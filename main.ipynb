{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48834b53-803c-4dfb-aa11-e0d5c792977e",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685275d5-526d-49ce-8127-6e29257ecc4e",
   "metadata": {},
   "source": [
    "A simple character-level RNN to generate new bits of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4debec-bb6f-4383-aa0a-a669ac1772b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: pytholic\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.6\n",
      "IPython version      : 8.16.1\n",
      "\n",
      "torch    : 2.1.0\n",
      "torchtext: 0.16.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"pytholic\" -v -p torch,torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b233f9-51cd-4572-8b98-7d8bdaba210f",
   "metadata": {},
   "source": [
    "Download the book *The Mysterious Island* by Jules Verne in plain text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b8def7-d647-43e9-9a77-7198fd4ff9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://www.gutenberg.org/files/1268/1268-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9eb22-9084-4760-a3ca-c0d01ce6b838",
   "metadata": {},
   "source": [
    "\n",
    "# Reading and preprocessing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006942f8-d068-4453-a746-f9f837f3e177",
   "metadata": {},
   "source": [
    "First we will read the text from the file and remove portions from the beginning and the end (these contain certain descriptions of the Gutenberg project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247930b7-c9ca-41e3-91a6-d40aae9edfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"1268-0.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    text = f.read()\n",
    "start_idx = text.find(\"THE MYSTERIOUS ISLAND\")\n",
    "end_idx = text.find(\"End of the Project Gutenberg\")\n",
    "text = text[start_idx:end_idx]\n",
    "char_set = set(text)  # removes duplicates -> get unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6ebc48-9f99-4828-8897-d8ecccd40b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length: 1130711\n",
      "Unique characters: 85\n"
     ]
    }
   ],
   "source": [
    "print(f\"Text Length: {len(text)}\")\n",
    "print(f\"Unique characters: {len(char_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872528fb-8d89-4ccc-821d-6228b3092844",
   "metadata": {},
   "source": [
    "Now we need to convert this data to numeric format. To do this, we will create a simple Python dictionary that maps each character to an integer. We will also need a reverse mapping to convert the results of our model back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3eec34b-d225-42d4-8fe2-b051129e6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "# int2chr = {i:ch for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted) # more efficient than dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f5f9fa-702b-4124-b458-9ad737e524a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1130711,)\n",
      "THE MYSTERIOUS ISLAND == Encoding ==> [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1 37 47 40 29 42 32]\n",
      "[48 36 33  1 41 53 47 48 33 46 37 43 49 47  1 37 47 40 29 42 32] == Reverse ==> THE MYSTERIOUS ISLAND\n"
     ]
    }
   ],
   "source": [
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "print(\"Text encoded shape: \", text_encoded.shape)\n",
    "print(f\"{text[:21]} == Encoding ==> {text_encoded[:21]}\")\n",
    "print(text_encoded[:21], \"== Reverse ==>\", \"\".join(char_array[text_encoded[:21]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842b2f6-c571-41e5-9698-e0e9e338acdf",
   "metadata": {},
   "source": [
    "For the text generation task, we can formulate the problem as a classification task. Since we have 85 unique characters, it becomes a *multiclass* classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9df989-e26d-4599-9311-97630091d9d1",
   "metadata": {},
   "source": [
    "We will clip the sequence length to 40. Longer length is better, but RNN will have problems capturing *long-range* dependencies. Sequence length is a hyperparameter optimization porblem, which we have to evaluate empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec3b705-d7ef-4e4c-b485-bfb33fe6ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2f7f0-a5e7-4bef-87ee-d2eb48cee2c4",
   "metadata": {},
   "source": [
    "We will split text into chunks of size 41. The first 40 character will form input sequence *x*, and the last 40 will form the taregt sequence *y* (since target is just offset by 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd9ffb6-53c2-4421-b863-7cd4cd43b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40 # hyperparameter\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded) - chunk_size+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a03ee3-8d6b-4a0d-a034-1f5485d728a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1], text_chunk[1:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7199be7-dfea-4e12-adba-663a2c38831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (x):  'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
      "Target (y):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "\n",
      "Input (x):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "Target (y):  'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3j/2s54wx4j0qsdwyxcpjx2h2780000gn/T/ipykernel_10566/3452713142.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = TextDataset(torch.tensor(text_chunks))\n",
    "\n",
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(\"Input (x): \", repr(\"\".join(char_array[seq])))\n",
    "    print(\"Target (y): \", repr(\"\".join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9f36b5-87d3-4dc1-b36c-176ac2c8e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into minibatches\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d882c-3119-463f-93f3-c2882f407192",
   "metadata": {},
   "source": [
    "The `drop_last` argument drops the last non-full batch of each worker’s iterable-style dataset replica i.e. the `drop_last=True` parameter ignores the last batch (when the number of examples in your dataset is not divisible by your `batch_size`) while `drop_last=False` will make the last batch smaller than your `batch_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4eaba8-5f97-433b-ab92-c1a8f3c6ab68",
   "metadata": {},
   "source": [
    "# Building a character-level RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f195fc-763c-4239-98f7-b1c618de052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d3fa2-4abf-473f-9a2b-b56eb88c2072",
   "metadata": {},
   "source": [
    "We are taking `logits` as output so that we can sample from the model predictions in order to generate new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b6ceeac-5ac0-4e05-a38a-9474ccf0690e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(85, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RNN model\n",
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdcfe15-aabf-4bce-9db9-cf6bc243e857",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f2d59f3-b212-44a0-a51a-d0f836f42421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c5822dc-a98b-4e87-b3a6-5cc30adc94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "torch.manual_seed(1)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    hidden, cell = hidden.to(device), cell.to(device)\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item() / seq_length\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch} loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62b042fb-21a1-45da-a8ae-86c40a8c912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e6b1fa3-7f84-4bb4-bdf8-0f9e6e5f8f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118032f-057d-4271-82c0-1e4ef48b807b",
   "metadata": {},
   "source": [
    "# Evaluation phase - generating new text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfb499-8d70-416a-b871-8ae8c62be4f5",
   "metadata": {},
   "source": [
    "The RNN model we trained in the previous section returnss the logits of size 85 for each unique character, which can be converted to probabilities via softmax. One simple way to generate text is to choose the character with the highest logit value. In other words, you pick the character that the model believes is the most likely next character based on its training.\n",
    "\n",
    "However, always selecting the character with the highest likelihood can result in repetitive and less interesting text. To make the text more diverse and less predictable, it's better to randomly sample from the model's output probabilities. This means you don't always choose the character with the highest probability; you make a random choice based on the distribution of probabilities.\n",
    "\n",
    "PyTorch already provides a class, `torch.distributions.categorical.Categorical`, which we can use to draw random samples from a categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17449673-b332-4129-8341-8eb3b4627c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def sample(model, starting_str, max_length=500, scale_factor=1.0):\n",
    "    \"\"\"\n",
    "    starting_str: short starting string\n",
    "    max_length: max length of generated text\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "    generated_str = starting_str # initially set it equal to the input str\n",
    "\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(batch_size=1)\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(max_length):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b97d6ce-d952-4005-b759-9b141e1cd340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island had built to push it disappeared, and for\n",
      "cut off!\n",
      "\n",
      "\n",
      "\n",
      "Charked during nature depped with himself at the entrme of Granite House might have been invited; must avoided against with his\n",
      "master, the two hung\n",
      "began to fire. The conversation of the kill Plencroft did not make sori.\n",
      "\n",
      "At the Gomering at the fauth\n",
      "of the chown of stutien on the destive chance of the latitude, and it was the chest of water\n",
      "and the retreats of the crater, out of lava and measures here, the sailor, “it is one,” replied Cyru\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str=\"The island\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407de96-e573-4e6e-a6c9-5ea5a9500fd4",
   "metadata": {},
   "source": [
    "The model msotly generates correct words. In some cases, sentences might also make sense. Further tuning the training parameters and model architecture can improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69901df-103e-4d19-a64c-2396088af5ba",
   "metadata": {},
   "source": [
    "# Temperate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a3cb6-7e29-4b71-8007-dd88a3fbac88",
   "metadata": {},
   "source": [
    "The `scale_factor` controls the randomness of the generated text. Low `scale_factor` (high temperature) results in more randomness because the output probability becomes more uniform, as opposed to more predictable behavior at high `scale_factor` (low temperature) where one logit will have high probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5128eddb-21ce-40c8-9401-86c0ddd002cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities before scaling: [0.10650698 0.10650698 0.78698605]\n",
      "Probabilities after scaling with 0.5: [0.21194156 0.21194156 0.57611686]\n",
      "Probabilities after scaling with 0.1: [0.3104238  0.3104238  0.37915248]\n",
      "Probabilities after scaling with 3.0: [0.00246652 0.00246652 0.9950669 ]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
    "print(f\"Probabilities before scaling: {nn.functional.softmax(logits, dim=1).numpy()[0]}\")\n",
    "print(f\"Probabilities after scaling with 0.5: {nn.functional.softmax(0.5 * logits, dim=1).numpy()[0]}\")\n",
    "print(f\"Probabilities after scaling with 0.1: {nn.functional.softmax(0.1 * logits, dim=1).numpy()[0]}\")\n",
    "print(f\"Probabilities after scaling with 3.0: {nn.functional.softmax(3.0 * logits, dim=1).numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3b756a7-1b49-4d81-bce3-8c3a0893551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was rumbled on the sea to the surface of the depths of the plateau, and he had probable to see the reporter was readily of the mouth of the lad, “there is a sandy disappeared. The captain and he was beginning, and the settlers would be a region of the corral.\n",
      "\n",
      "Cyrus Harding had resembled in a hollow of which they had been still seen to the result of sand. The island was the extent of a convicts were the reporter, “we will save him, will not help like a day, the settlers one!” replied Herbert, s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str=\"The island\", scale_factor=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e9f98f1-1603-4590-b2e2-cf12f1ab0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island\n",
      "addsh\n",
      "iron me usual, dixed lided, “you\n",
      "arragt frours\n",
      "ond,rlesides:-1,\n",
      "driving bedubely\n",
      "admasted\n",
      "amordiceneim: direfimed Cape.\n",
      "\n",
      "mb washed firl, invimniable afta. Red, The “Tswesturge\n",
      "Fally an’clets, tbE!\n",
      "Thout, vieh for\n",
      "been, Ayrton, “is, ofw two-thour,\n",
      "grauts!\n",
      "evey wisherriiefsure!’”\n",
      "“Let if they unlay,”\n",
      "\n",
      "rat’s fegle?”\n",
      "Not\n",
      "tie, one or deaver. Chance spilacquatity alludelictle of all eaght; Pencroft? Las rithinkswest at\n",
      "not admisic-ioved, been made Snall fiormed.\n",
      "How\n",
      "stop!”\n",
      "\n",
      "Herbert, light! pros\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str=\"The island\", scale_factor=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb305ccd-02d9-4669-a5a6-821f79e3b54d",
   "metadata": {},
   "source": [
    "As we can see, the results are coherent with our hypothesis. We can choose to generate correct text with less novelty or create diverse text with mroe randomness. It is a trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15a5a4-f845-4820-9f90-a6c9644a7534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
